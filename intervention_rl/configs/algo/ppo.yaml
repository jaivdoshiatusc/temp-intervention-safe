name: ppo  # algorithm name

ppo:
  policy: "MlpPolicy"               # Policy model to use (MlpPolicy, CnnPolicy, ...)
  policy_kwargs: {}                 # Additional arguments for policy model
  
  learning_rate: 0.0001             # Learning rate for A2C
  n_steps: 512                      # Number of steps in rollout
  batch_size: 16                    # Minibatch size
  n_epochs: 4                       # Number of optimization epochs
  gamma: 0.999                      # Discount factor
  gae_lambda: 0.99                  # GAE (Generalized Advantage Estimation) lambda
  clip_range: 0.1                   # Clipping parameter, constant c in PPO
  clip_range_vf: None               # Clipping parameter for the value function, constant c in PPO
  normalize_advantage: true         # Normalize advantage estimates
  ent_coef: 0.01                    # Entropy coefficient
  vf_coef: 0.5                      # Value function coefficient
  max_grad_norm: 0.5                # Max norm for gradient clipping
  use_sde: false                    # Use State Dependent Exploration (SDE)
  sde_sample_freq: -1               # SDE sampling frequency
  verbose: 1                        # Verbosity level
